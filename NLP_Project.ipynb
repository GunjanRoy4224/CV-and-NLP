{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKqrg-KHJ4sB"
   },
   "source": [
    "NLP Track Capstone Project: Evaluation of Multilingual Models\n",
    " BERT (monolingual) with XLM-RoBERTa (multilingual)\n",
    "on sentiment classification across multiple languages.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Author: Gunjan Kumar\n",
    "\n",
    "Date: 4 November 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1762282273897,
     "user": {
      "displayName": "Gunjan Roy",
      "userId": "03023376962891594685"
     },
     "user_tz": -330
    },
    "id": "Z6O2jNXmKSlW",
    "outputId": "eeb253a3-caee-40d7-fe53-d7e6eb0318fd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"imports successful!\")\n",
    "print(f\"Using device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1762282277722,
     "user": {
      "displayName": "Gunjan Roy",
      "userId": "03023376962891594685"
     },
     "user_tz": -330
    },
    "id": "0MdeOnvHLmqP"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def load_multilingual_amazon_reviews():\n",
    "\n",
    "    print(\"Loading Multilingual Reviews Dataset (synthetic fallback)\")\n",
    "\n",
    "    languages = ['en', 'de', 'fr', 'es', 'ja']\n",
    "    language_names = {\n",
    "        'en': 'English',\n",
    "        'de': 'German',\n",
    "        'fr': 'French',\n",
    "        'es': 'Spanish',\n",
    "        'ja': 'Japanese'\n",
    "    }\n",
    "\n",
    "    datasets = {}\n",
    "    # Sample review templates per language\n",
    "    samples = {\n",
    "        'en': {\n",
    "            'positive': [\"Amazing product!\", \"Works perfectly!\", \"Loved it!\"],\n",
    "            'neutral': [\"It's okay.\", \"Average quality.\", \"Not bad.\"],\n",
    "            'negative': [\"Very poor.\", \"Terrible experience.\", \"Not worth it.\"]\n",
    "        },\n",
    "        'de': {\n",
    "            'positive': [\"Tolles Produkt!\", \"Funktioniert perfekt!\", \"Ich liebe es!\"],\n",
    "            'neutral': [\"Es ist okay.\", \"Durchschnittliche Qualität.\", \"Nicht schlecht.\"],\n",
    "            'negative': [\"Sehr schlecht.\", \"Schreckliche Erfahrung.\", \"Nicht empfehlenswert.\"]\n",
    "        },\n",
    "        'fr': {\n",
    "            'positive': [\"Produit incroyable!\", \"Fonctionne parfaitement!\", \"Je l'adore!\"],\n",
    "            'neutral': [\"C'est correct.\", \"Qualité moyenne.\", \"Pas mal.\"],\n",
    "            'negative': [\"Très mauvais.\", \"Mauvaise expérience.\", \"Je ne recommande pas.\"]\n",
    "        },\n",
    "        'es': {\n",
    "            'positive': [\"¡Producto increíble!\", \"Funciona perfectamente!\", \"¡Me encanta!\"],\n",
    "            'neutral': [\"Está bien.\", \"Calidad promedio.\", \"No está mal.\"],\n",
    "            'negative': [\"Muy malo.\", \"Experiencia terrible.\", \"No lo recomiendo.\"]\n",
    "        },\n",
    "        'ja': {\n",
    "            'positive': [\"素晴らしい製品！\", \"完璧に動作します！\", \"気に入りました！\"],\n",
    "            'neutral': [\"まあまあです。\", \"平均的な品質です。\", \"悪くないです。\"],\n",
    "            'negative': [\"とても悪い。\", \"ひどい経験。\", \"おすすめしません。\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Generate random synthetic data\n",
    "    for lang in languages:\n",
    "        data = []\n",
    "        for _ in range(300):  # 300 samples per language\n",
    "            sentiment = np.random.choice(['positive', 'neutral', 'negative'])\n",
    "            label = 2 if sentiment == 'positive' else (1 if sentiment == 'neutral' else 0)\n",
    "            text = np.random.choice(samples[lang][sentiment])\n",
    "            data.append({'text': text, 'label': label, 'language': lang})\n",
    "        datasets[lang] = Dataset.from_pandas(pd.DataFrame(data))\n",
    "\n",
    "    print(f\"Successfully created synthetic dataset with {len(languages)} languages.\")\n",
    "    return datasets, language_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1762282278823,
     "user": {
      "displayName": "Gunjan Roy",
      "userId": "03023376962891594685"
     },
     "user_tz": -330
    },
    "id": "ePzTZxx1C3lV"
   },
   "outputs": [],
   "source": [
    "languages = ['en', 'de', 'fr', 'es', 'ja']\n",
    "language_names = {\n",
    "        'en': 'English',\n",
    "        'de': 'German',\n",
    "        'fr': 'French',\n",
    "        'es': 'Spanish',\n",
    "        'ja': 'Japanese'\n",
    "    }\n",
    "datasets = {}\n",
    "for lang in languages:\n",
    "        data = []\n",
    "        for _ in range(300):  # 300 samples per language\n",
    "            sentiment = np.random.choice(['positive', 'neutral', 'negative'])\n",
    "            label = 2 if sentiment == 'positive' else (1 if sentiment == 'neutral' else 0)\n",
    "            text = np.random.choice(samples[lang][sentiment])\n",
    "            data.append({'text': text, 'label': label, 'language': lang})\n",
    "\n",
    "        datasets[lang] = Dataset.from_pandas(pd.DataFrame(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1762282279874,
     "user": {
      "displayName": "Gunjan Roy",
      "userId": "03023376962891594685"
     },
     "user_tz": -330
    },
    "id": "DbfcADRUDfq4",
    "outputId": "fc4a1aaf-2299-48c9-fa46-e957113e1fcb"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "datasets, language_names = load_multilingual_amazon_reviews()\n",
    "\n",
    "# Display dataset statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "for lang, ds in datasets.items():\n",
    "    label_dist = pd.Series([x['label'] for x in ds]).value_counts().sort_index()\n",
    "    print(f\"{language_names[lang]:10s} | Samples: {len(ds):4d} | Labels: {dict(label_dist)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1762282282494,
     "user": {
      "displayName": "Gunjan Roy",
      "userId": "03023376962891594685"
     },
     "user_tz": -330
    },
    "id": "568Xv8TQHCzX",
    "outputId": "ae7ad99c-53f4-4dc1-eead-d1269a1e247e"
   },
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self, model_name, is_multilingual=False):\n",
    "        self.model_name = model_name\n",
    "        self.is_multilingual = is_multilingual\n",
    "        self.num_labels = 3  # Negative, Neutral, Positive\n",
    "        self.max_length = 128\n",
    "        self.batch_size = 16\n",
    "        self.learning_rate = 2e-5\n",
    "        self.num_epochs = 3\n",
    "        self.weight_decay = 0.01\n",
    "\n",
    "# Define models to compare\n",
    "models_config = {\n",
    "    'monolingual': ModelConfig('bert-base-uncased', is_multilingual=False),\n",
    "    'multilingual': ModelConfig('xlm-roberta-base', is_multilingual=True)\n",
    "}\n",
    "\n",
    "print(\"Model Configurations:\")\n",
    "\n",
    "for name, config in models_config.items():\n",
    "    print(f\"{name.upper()}\")\n",
    "    print(f\"  Model: {config.model_name}\")\n",
    "    print(f\"  Type: {'Multilingual' if config.is_multilingual else 'Monolingual'}\")\n",
    "    print(f\"  Labels: {config.num_labels}\")\n",
    "    print(f\"  Max Length: {config.max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1762282284425,
     "user": {
      "displayName": "Gunjan Roy",
      "userId": "03023376962891594685"
     },
     "user_tz": -330
    },
    "id": "FdJ76_LOHXxC"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(datasets, tokenizer, max_length=128):\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    tokenized_datasets = {}\n",
    "    for lang, ds in datasets.items():\n",
    "        tokenized_datasets[lang] = ds.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=['text', 'language']\n",
    "        )\n",
    "        tokenized_datasets[lang].set_format('torch')\n",
    "\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1762282466336,
     "user": {
      "displayName": "Gunjan Roy",
      "userId": "03023376962891594685"
     },
     "user_tz": -330
    },
    "id": "wX14nwzGHdnc"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainerCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Optional progress print callback\n",
    "class PrintCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        print(f\"Starting epoch {state.epoch}\", flush=True)\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        print(f\"✅ Finished epoch {state.epoch}\\n\", flush=True)\n",
    "\n",
    "def train_model(model_config, train_dataset, eval_dataset, output_dir):\n",
    "    print(f\"Training model: {model_config.model_name}\")\n",
    "    print(\"=\" * 60, flush=True)\n",
    "\n",
    "    # Load model + tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_config.model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_config.model_name,\n",
    "        num_labels=model_config.num_labels\n",
    "    )\n",
    "\n",
    "    # Tokenize data\n",
    "    train_data = preprocess_data({'train': train_dataset}, tokenizer, model_config.max_length)['train']\n",
    "    eval_data = preprocess_data({'eval': eval_dataset}, tokenizer, model_config.max_length)['eval']\n",
    "\n",
    "    # ✅ Version-safe training arguments (no eval/save strategy conflict)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=model_config.batch_size,\n",
    "        per_device_eval_batch_size=model_config.batch_size,\n",
    "        num_train_epochs=model_config.num_epochs,\n",
    "        learning_rate=model_config.learning_rate,\n",
    "        weight_decay=model_config.weight_decay,\n",
    "        logging_steps=50,\n",
    "        save_total_limit=1,\n",
    "        # Removed: load_best_model_at_end to avoid strategy conflict\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        return {\n",
    "            \"accuracy\": accuracy_score(labels, preds),\n",
    "            \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "        }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "        callbacks=[PrintCallback()]\n",
    "    )\n",
    "\n",
    "    print(\"Beginning training\\n\", flush=True)\n",
    "    trainer.train()\n",
    "    print(f\"Training completed successfully for {model_config.model_name}\\n\", flush=True)\n",
    "\n",
    "    return model, tokenizer, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1762282287680,
     "user": {
      "displayName": "Gunjan Roy",
      "userId": "03023376962891594685"
     },
     "user_tz": -330
    },
    "id": "6xnK1B5UHwdP"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, test_datasets, max_length=128):\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for lang, ds in test_datasets.items():\n",
    "        print(f\"Evaluating on {language_names[lang]}...\")\n",
    "\n",
    "        # Tokenize\n",
    "        tokenized = preprocess_data({lang: ds}, tokenizer, max_length)[lang]\n",
    "\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(tokenized)):\n",
    "                inputs = {\n",
    "                    'input_ids': tokenized[i]['input_ids'].unsqueeze(0).to(device),\n",
    "                    'attention_mask': tokenized[i]['attention_mask'].unsqueeze(0).to(device)\n",
    "                }\n",
    "                outputs = model(**inputs)\n",
    "                pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "                predictions.append(pred)\n",
    "                true_labels.append(tokenized[i]['label'].item())\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "        conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "        results[lang] = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'predictions': predictions,\n",
    "            'true_labels': true_labels\n",
    "        }\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f} | F1-Score: {f1:.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1762282289780,
     "user": {
      "displayName": "Gunjan Roy",
      "userId": "03023376962891594685"
     },
     "user_tz": -330
    },
    "id": "mE8yn68GIwPu"
   },
   "outputs": [],
   "source": [
    "def plot_performance_comparison(results_mono, results_multi):\n",
    "    \"\"\"\n",
    "    Plot performance comparison for monolingual and multilingual models\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Model Performance Comparison Across Languages', fontsize=16, fontweight='bold')\n",
    "\n",
    "    languages_list = list(language_names.keys())\n",
    "    lang_labels = [language_names[l] for l in languages_list]\n",
    "\n",
    "    # Extract metrics\n",
    "    mono_acc = [results_mono[l]['accuracy'] for l in languages_list]\n",
    "    multi_acc = [results_multi[l]['accuracy'] for l in languages_list]\n",
    "    mono_f1 = [results_mono[l]['f1'] for l in languages_list]\n",
    "    multi_f1 = [results_multi[l]['f1'] for l in languages_list]\n",
    "\n",
    "    # Plot 1: Accuracy comparison\n",
    "    x = np.arange(len(lang_labels))\n",
    "    width = 0.35\n",
    "    axes[0, 0].bar(x - width/2, mono_acc, width, label='BERT (Mono)', alpha=0.8, color='steelblue')\n",
    "    axes[0, 0].bar(x + width/2, multi_acc, width, label='XLM-R (Multi)', alpha=0.8, color='seagreen')\n",
    "    axes[0, 0].set_ylabel('Accuracy', fontweight='bold')\n",
    "    axes[0, 0].set_title('Accuracy Comparison', fontweight='bold')\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(lang_labels, rotation=45)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    axes[0, 0].set_ylim([0, 1])\n",
    "\n",
    "    # Plot 2: F1-Score comparison\n",
    "    axes[0, 1].bar(x - width/2, mono_f1, width, label='BERT (Mono)', alpha=0.8, color='steelblue')\n",
    "    axes[0, 1].bar(x + width/2, multi_f1, width, label='XLM-R (Multi)', alpha=0.8, color='seagreen')\n",
    "    axes[0, 1].set_ylabel('F1-Score', fontweight='bold')\n",
    "    axes[0, 1].set_title('F1-Score Comparison', fontweight='bold')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(lang_labels, rotation=45)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    axes[0, 1].set_ylim([0, 1])\n",
    "\n",
    "    # Plot 3: Performance difference (Multi - Mono)\n",
    "    acc_diff = [m - mo for m, mo in zip(multi_acc, mono_acc)]\n",
    "    colors = ['green' if d > 0 else 'red' for d in acc_diff]\n",
    "    axes[1, 0].bar(lang_labels, acc_diff, color=colors, alpha=0.7)\n",
    "    axes[1, 0].set_ylabel('Accuracy Difference', fontweight='bold')\n",
    "    axes[1, 0].set_title('Multilingual Advantage (XLM-R - BERT)', fontweight='bold')\n",
    "    axes[1, 0].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    plt.setp(axes[1, 0].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # Plot 4: Line plot showing trend\n",
    "    axes[1, 1].plot(lang_labels, mono_acc, marker='o', label='BERT (Mono)', linewidth=2, markersize=8)\n",
    "    axes[1, 1].plot(lang_labels, multi_acc, marker='s', label='XLM-R (Multi)', linewidth=2, markersize=8)\n",
    "    axes[1, 1].set_ylabel('Accuracy', fontweight='bold')\n",
    "    axes[1, 1].set_title('Accuracy Trend Across Languages', fontweight='bold')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].set_ylim([0, 1])\n",
    "    plt.setp(axes[1, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrices(results, model_name):\n",
    "\n",
    "    print(f\"Plotting confusion matrices for {model_name} model...\")\n",
    "    n_langs = len(results)\n",
    "    fig, axes = plt.subplots(1, n_langs, figsize=(4*n_langs, 4))\n",
    "    fig.suptitle(f'Confusion Matrices - {model_name}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    if n_langs == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, (lang, res) in enumerate(results.items()):\n",
    "        cm = res['confusion_matrix']\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                   xticklabels=['Neg', 'Neu', 'Pos'],\n",
    "                   yticklabels=['Neg', 'Neu', 'Pos'])\n",
    "        axes[idx].set_title(language_names[lang], fontweight='bold')\n",
    "        axes[idx].set_ylabel('True Label')\n",
    "        axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'confusion_matrices_{model_name.replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "839296406d0745e99619e8ade516e0b4",
      "c155fd3ab3304d6e8c93555e15c65d48",
      "8ed0dfb3f4e64f03b5930574a9e80ba9",
      "d8571b33d8404b849993b7b65fd15114",
      "885037f44d694e869b0fc1fc4b67a3ae",
      "8d7a81134e3b414c855e1575273e55eb",
      "02e0eec1b9d5447c9c1dd9402060c755",
      "14995bea5133493d929d310d201ea0e4",
      "a0ba8714ce83454a86659d7f07ffc8b1",
      "0d6fa606a2474ccb9a641198e45bb967",
      "5bd892dfbb584e1687eb167074dc2d2c",
      "6d1df5ecefa74ca8b9ee36cc11e02d6f",
      "0fb068ab907c41fb8965e1249fd64a49",
      "c40025f1ec47480f86fcedd224f085b4",
      "5755c6649e9d4056a7485d512e550b69",
      "870f92baf0ec4b46a02c99a3e1f8ee51",
      "9cfd27333b234a11a5cd3cf077cd89a8",
      "77116cee48f740b9a5755a2be4fc246c",
      "54f2692619d247dfa9cc0600b17af933",
      "54e0dc7f2cf44463930a2bcf5464f020",
      "aa58e27f8f5d46f2adab3aa14ad68afb",
      "303f90f83cb044bda1b13e672e2b90d3",
      "1a3f841426c14ab082705b56b1d5061e",
      "b46196c9366b48938ab95f4bf8aab31a",
      "7f9d236006474edc9af3fc0c3b323d5e",
      "d0b195ef46db4513b446f6db5e4024bf",
      "a88444e31ca3400da9fb1cbd7bf1df42",
      "5bdface746274e94a917701932ed93e2",
      "5cbddd6fb6b94c8f9dbf5b3ac33cec5c",
      "aa460e8ec0d64168a6ba1540e8eb7659",
      "39db6d27efea4ba087abf64c7f8c33e8",
      "d2511095991d44c1b0acacfc502dcb8c",
      "f71680288cb74837abc9579a3949a0f6",
      "b98a7e2cd5284aca80bd224e6927ca5e",
      "613a5e8cd1844e33bd100e819890ee3b",
      "c6cd85c668834e3491dc87d1b5236cb0",
      "e3c27b9c08994de79bf8de0a41cf7841",
      "99a89e4eb0d14f1a943f76c9052f5598",
      "7b33e5601b6c4f208164f53ea99f1536",
      "b0a78fa9fabd436d997877ca8aa2fc8a",
      "bc7ab894889d4f6b8ccffbff0089b92b",
      "9a25ff5ae16d480dbf7506dc4b8564d3",
      "18675d31c17d4397ad9bcb0ae35756d8",
      "2bae633a1a8e4f2a884bf6aa49c4f443",
      "1f45dea1aeb843cfa0eecfe33b9301dd",
      "008924c6ecdb43e7b09a27c03f0c76a6",
      "c6b2c8bff98c4c7ca6276430d0b6280f",
      "2a6279517e25486f83c0360637f1217c",
      "d3533460a5f747dba49fe34eb0fc5acf",
      "beea9db61a4c472d83b58b3e60e8bea4",
      "f76896d4b31d4a74baa3aea8002e5536",
      "6937a43b3a1a43df92a5908ee2ef13e1",
      "7a21a3f8aa504e36a486179c6ec938e7",
      "f9bedde2a3914f61bfc93b41fce1d06e",
      "63edf4ebe2ae4d9a92a706f4f6e15c5b",
      "16564f830b424def8f5c967c594aa326",
      "012ab18ab0984d3aaa6a379bf9aa82b3",
      "8e8c77f90df74967a05a4cfdb805cf21",
      "c42f686a055f4f9ba54c8a60d5baa748",
      "f3841eb3b62a44649058a4008e0740c4",
      "370a925ccc534a7f934d27b49a47871a",
      "7f47454d4ef241d2bafee7d5815ab7ef",
      "398f35597d04455e98bd71f269b4b149",
      "1488066c846748dba67837b0e726ecd4",
      "aa079cb4e6a74eba8644b28b75b8bafb",
      "95e7d4fd798549709e1b8cac6fa5dba3",
      "f7beed13f0f9466095e997b111ee0c53",
      "bbab6d90d4604d29801b64e0da23dfd9",
      "f1ccb11c9e5c4b3092b7f0739cdc9831",
      "d348b9fb38844e3e981f86259c394c4c",
      "df99897d8c2a4c38867e0ed49a5d9e4c",
      "7b1e7bd9eeed49178f097549622d931e",
      "d0b6953f9b9141f0a8d8c8a82a1afe99",
      "060f651094854ee29832540911d12efb",
      "09483bd37d0645f1a30dd6c6abb581d9",
      "bb7bb104d9614208a5c08c1629288ef9",
      "56bee744832942cf901fe9e9e80eaa4a",
      "0ff3c1c105a248fea986daf1a92b9a68",
      "f1d7ec21db6c42c3a4230e03273a1a2c",
      "b1b20d1b76b44347af6110dcb90bd6b7",
      "b5a5328cc848463e85eb9cb52d829c90",
      "00425d0cf90d4fb59051c53315cb4b9e",
      "2a4d53e94988436d9eda864545da53a3",
      "62f27c35daaf41389d2862e65195c7b6",
      "c49f050915d04096a2026e3f4e6ef6e9",
      "1b10cf17e8234242baf8b68670a80023",
      "10caa088ff864c5483ffdb3fb1e9a01d",
      "74e922b6d3e24043b1850e0488703e45",
      "6cdf1e22bd954d488e7c24c5b8de290d",
      "cb14813652ee421a9f951dfbdeb9e1e2",
      "fd954cb9ba8348ea84bc8df50aca569f",
      "58a953e25d264e648a6f703301f5903b",
      "10bca6f51f734597ab46d5e96c1a867d",
      "875b7391debf4234a4486d735e2ec11b",
      "003b219c087149baa6d52621f4f86bea",
      "e9305be4ffa54b6db6d637d851ac15da",
      "08d45b980be344018de3ed665420e33c",
      "251215df4fac463bb37265183dcd0444",
      "f8029aaacd4146e2a0233bb9c1affa8e",
      "029a2039d71d4f94bdc55aff95fbf514",
      "a3c65230dd85457ea9b98408c64a15e0",
      "164aac8d051741d182748ba02539ca3a",
      "36c5ae5af72b4a80a2cac122f94a131b",
      "7cecbed6652b4a7eac4578e1274d4e6a",
      "2b25aa15073a43e2aae1279350f90e2c",
      "f7906c53b0f84057bf5bca14801d0cb7",
      "0c2c2528721f450da51365617f469098",
      "310b4c29ff224987922cbf2fe71e99a1",
      "5b8e878faa6149c4b4b0dabd18ee7fa0",
      "e2b74afd25a748f8876989b7329842f2",
      "46d866d689924bc1b7c2df633c3aaa6c",
      "c4fc541eded74aceae1ecea83faecdfa",
      "ad2ead069d0f4f95a7529243b28068e7",
      "e61e12f3429b4eaf8bb187ef0911c6c7",
      "31612d94f0a648828adb0c2b79335289",
      "76179177e66a486dacee1a6bd793a789",
      "518da78099884c878a14e5fa615632f2",
      "44aaeecd80cc445e899ecd127b779304",
      "7afd157a024345b99238041cc38b0237",
      "f8d9cab07bc04b299f99d3d0a9b5b9c0",
      "d478205343eb4f7982f89f137a3a30cc",
      "f4625e4536a343d98801da12ad7c8d13",
      "0816cd0572274b48956ee1c334016c2e",
      "5044d3065f8049488e5d2673e009df31",
      "afb41899d8a243ca932f2da68f71f763",
      "3bb5ccfc508a488a9100d9c10fe18245",
      "c3e346e5da61431aa0985743695b3d4c",
      "f5cdc05b0e5d48a185bee95bfe39d922",
      "4157a6a505f34dacac0f0d11b0a4e233",
      "7a3cb7f72b5746cfb1d6fc37ca20267e",
      "1415f362e2884dcd95e6b695a13e1f8f",
      "d1f7e39587a34c8091f50e268ce60170",
      "a1e29a688f3c44b08faff4e458f810be",
      "8a3f88442fb3431d9ba7fc17998fa84f",
      "846565479c47452495320d7e570d94a4",
      "3080d128759f444b8ee22e8863d804a5",
      "0f122dcfa315444f8828695573f2d4db",
      "511ff13dd5ad43f1a3df2e274371e62d",
      "3c63ce2f07844369bd0ee7b59476cdda",
      "511752c3b40d4d97a5646e7289ce3aba",
      "b510b8d26d2540bdac4971cce8d90944",
      "22ec67193cfa44269c80e1b95c82f6bd",
      "785c7d98c9a34a37bab54e708560d5b2",
      "263ffd87c0e948fbb5c5caa8937b9cc9",
      "8b2cad70ab3546078a03f2e8004b5ef1",
      "90049021960349dfafca586e5901f369",
      "859bd86ca29c45a4b17d0ce66ce7bacc",
      "deeb496fc4f04f45abc95832f2c6fb2a",
      "cb3042a339954d9c9da7faf336eb3ba0",
      "4088ddda23d44af0a6f2ed4088c06970",
      "ed7e4cf033c14093a02d0551afbcc890",
      "b896119d48c2488eb9965862f313b66e",
      "0028ce7c0d9f45d68ca491675ee0a91c",
      "16a93e90873e4e769a057d7094ff5030",
      "6876d6d416214b4cba70f1bd55a8d227",
      "a18c0c19115a4ba0b99ef0e62276003b",
      "1a06dbaa01c7483698f8aeb4002eef37",
      "0009d793c84b4670a10adbe2f25d3f36",
      "6075499fda9841239eaea519f4b60959",
      "55b3ccf0c0a14ada84e3626bd9c00749",
      "689eaa51a69e408fab701241970c5caf",
      "e232216b203b46dba3b43ad5048712ec",
      "a7af1ab369c84b5abf1eb62dea0d52fa",
      "0ec65fe33419481eba816b5089cc1b67",
      "0d645c257d9f417d8c173df9d8732bdc",
      "5b52618a55104a309ee18e95982cded5",
      "f2706d3c1cb243c3acaa777fb74596fe",
      "f0b0379e4d544fafb8fff7cc4cc8ae10",
      "ed9c7962944e42cf9ca9f43c9afa2581",
      "f87eb51a1e0f4f1e908d1a1ac7915fa3",
      "2283bff1cc714d0682f63d8cb41d0868",
      "48ea68e3d3194f17aed3cdf7ddcede1e",
      "7480146662c04858880690c99865b3aa",
      "4775729b8e14466e895248e6dbdf6ccf",
      "4911033bcbe94e67acaf816bad1db396",
      "ba7b2c1425184001b3bdd94f91f24ba0",
      "b5d2cf0698df4ac2906b7e818c6c0dcc",
      "6efab061a06243158d20d76d686078ed",
      "9b9352b7893f45c7a0a6c5f901d267c3",
      "81e1c4a19e6f4cfc9f988471f7062bf5",
      "aba54013c88c4729a99cb1e0a0e027c5",
      "bd1830b8815b4d4f9a2f1547496961d9",
      "a381106829084584868c3cbf8bd5e703",
      "9b7f3908808c4b3288ed7770979ea5ba",
      "045f6bbf546949088d9d48306c6f7344",
      "d7531156535c43289c135de523b529e8",
      "6e41add463944d0f8f164d33900beaed",
      "405d1bb370694019866357fd9f4f609a",
      "ede38a6f1de1415e8bbc2e518e988457",
      "b1e3c791cf0a4e448e6f70ec3bbb8f0f",
      "4740593926ad4195aa35123283507713",
      "2dd2f29216944dcd8b4043c2ac658b32",
      "2f55220407404cb7b09959f2ec2a09d7",
      "fadc63c4eefe4ec99862c3cdbb1ceaa3",
      "51af447eab004329a0bcd015413a866b",
      "f8f4fede1e2c40689ece112670325ff4",
      "682bd897b2d34c6ba802c2659cf9914f",
      "23eebde3e84344e89fdc714a254035ce",
      "4bb9bebbade24d499da553477a5f3bc4",
      "71dec74cbb3c4a04bb168e01d388ec7a",
      "f83df4b337b648c4b01a5dca35eef29d",
      "9453eeb3f4dc436795c7c4c1627f9b8f",
      "a9ab9891e1534af29ca1d82615eda4b3",
      "44ef6224b6d44e32ba0eb803d120ea79",
      "fa2f18ba4af0460f967ba561f158dc78",
      "0292069650934d53b41c785543f8c57e",
      "74990019c5c44a00bfe5848af04fd52b",
      "69fcc33fb793412b955818ccce6a6639",
      "299a65de7b40467cb51077b00e72c65c"
     ]
    },
    "executionInfo": {
     "elapsed": 1364793,
     "status": "ok",
     "timestamp": 1762283836156,
     "user": {
      "displayName": "Gunjan Roy",
      "userId": "03023376962891594685"
     },
     "user_tz": -330
    },
    "id": "Kk4l977vI4bU",
    "outputId": "588a0d08-2ea1-4461-b98f-70bd7bc975ed"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NLP CAPSTONE: MULTILINGUAL MODEL EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Prepare train/test split (train on English, test on all)\n",
    "    train_dataset = datasets['en']\n",
    "    test_datasets = {lang: ds for lang, ds in datasets.items()}\n",
    "    eval_dataset = datasets['en'].select(range(min(200, len(datasets['en']))))\n",
    "\n",
    "    results_all = {}\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    for model_type, config in models_config.items():\n",
    "\n",
    "        print(f\"  PROCESSING: {model_type.upper()} MODEL\")\n",
    "\n",
    "\n",
    "        output_dir = f\"./results/{model_type}\"\n",
    "\n",
    "        # Train model\n",
    "        model, tokenizer, trainer = train_model(\n",
    "            config,\n",
    "            train_dataset,\n",
    "            eval_dataset,\n",
    "            output_dir\n",
    "        )\n",
    "\n",
    "        # Evaluate on all languages\n",
    "        print(f\"Evaluating {model_type} model across languages\")\n",
    "        results = evaluate_model(model, tokenizer, test_datasets, config.max_length)\n",
    "        results_all[model_type] = results\n",
    "\n",
    "        # Plot confusion matrices\n",
    "        plot_confusion_matrices(results, f\"{model_type.capitalize()} Model\")\n",
    "\n",
    "    # Compare models\n",
    "    print(\"  FINAL COMPARISON\")\n",
    "    plot_performance_comparison(results_all['monolingual'], results_all['multilingual'])\n",
    "\n",
    "    # Print summary table\n",
    "    print(\"Performance Summary:\")\n",
    "\n",
    "    print(f\"{'Language':<12} {'Monolingual Acc':<18} {'Multilingual Acc':<18} {'Difference':<12}\")\n",
    "    for lang in language_names.keys():\n",
    "        mono_acc = results_all['monolingual'][lang]['accuracy']\n",
    "        multi_acc = results_all['multilingual'][lang]['accuracy']\n",
    "        diff = multi_acc - mono_acc\n",
    "        print(f\"{language_names[lang]:<12} {mono_acc:>16.4f} {multi_acc:>18.4f} {diff:>+11.4f}\")\n",
    "\n",
    "\n",
    "    return results_all\n",
    "\n",
    "# Run the main pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()\n",
    "    print(\"Project completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN9o+AmHmGu3sW0glnupE55",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
